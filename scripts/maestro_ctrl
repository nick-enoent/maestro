#!/usr/bin/env python3
import os, sys
import errno
import yaml
import argparse
import json
import subprocess
import socket
import time
import itertools as it
from collections.abc import Mapping, Sequence
from ldmsd.ldmsd_parser import *

class ClusterCtrl(object):
    def emit_value(self, path, value):
        try:
            res = client.put(path, str(value))
        except Exception as e:
            print("Error {0} setting {1} : {2}".format(str(e), path, str(value)))

    def check_key(self, key):
        # Handle forward slash in keys e.g. endpoints containing "/" in the name
        try:
            if '/' in key:
                print(f'Error: "/" is not a supported character in key name {key}')
                sys.exit(1)
            return key
        except Exception as e:
            print(str(e))

    def walk(self, obj, path=''):
        if obj is None:
            if path.split("/")[-1] in CORE_ATTRS:
                print(f'{path.split("/")[-1]} not present in ldms yaml configuration file.\nContinuing..')
        elif isinstance(obj, Mapping):
            for key in obj:
                safe_key = self.check_key(key)
                self.walk(obj[key], '{0}/{1}'.format(path, safe_key))
        elif isinstance(obj, Sequence):
            if isinstance(obj, (str, bytearray)):
                self.emit_value(path, obj)
            else:
                item = 0
                for v in obj:
                    # we want keys to be returned in numerical order which requires z-fill
                    self.walk(v, path + '/{0:06}'.format(item))
                    item += 1
        elif obj:
            self.emit_value(path, obj)

    def __init__(self, client, name, cluster_config, args):
        self.client = client
        self.name = name
        self.args = args
        self.cluster_config = cluster_config
        self.yaml_parser = YamlParser(self.name, cluster_config, self.args)
        self.yaml_parser.build_all()
        self.daemons = self.yaml_parser.daemons
        self.aggregators = self.yaml_parser.aggregators
        self.producers = self.yaml_parser.producers
        self.updaters = self.yaml_parser.updaters
        self.stores = self.yaml_parser.stores
        self.samplers = self.yaml_parser.samplers

    def commit(self):
        pass

    def save_config(self):
        try:
            self.client.delete_prefix('/' + self.name)
            self.walk(self.daemons, '/' + self.name + '/daemons')
            self.walk(self.aggregators, '/' + self.name + '/aggregators')
            self.walk(self.producers, '/' + self.name + '/producers')
            self.walk(self.updaters, '/' + self.name + '/updaters')
            self.walk(self.stores, '/' + self.name + '/stores')
            self.walk(self.samplers, '/' + self.name + '/samplers')
            self.client.put('/'+self.name+'/last_updated', str(time.time()))
        except Exception as e:
            a, b, c = sys.exc_info()
            print(str(e)+' '+str(c.tb_lineno))
            return 1

    def local_mode(self, local_path):
        # Local mode uses hostname to help identify which daemon(s) to start
        hostname = socket.gethostname()
        local_list = {}
        fd = None
        match_host = False
        for dmn_grp in self.daemons:
            for dmn in self.daemons[dmn_grp]:
                auth_list = {}
                if hostname == self.daemons[dmn_grp][dmn]['addr']:
                    match_host = True
                    local_list[dmn] = self.daemons[dmn_grp][dmn]
                    local_list[dmn]['dmn_grp'] = dmn_grp
                    if dmn_grp in self.aggregators and dmn in self.aggregators[dmn_grp]:
                        try:
                            fd = open(f'{local_path}/{dmn}.conf', 'w+')
                            self.write_listeners(fd, dmn_grp, dmn, auth_list)
                            self.write_producers(fd, dmn_grp, dmn, auth_list)
                            self.write_stream_subscribe(fd, dmn_grp, dmn)
                            self.write_agg_plugins(fd, dmn_grp, dmn)
                            self.write_updaters(fd, dmn_grp)
                            self.write_stores(fd, dmn_grp)
                        except Exception as e:
                            print(f'Error {e}: writing ldms configuration files')
                    if fd:
                        fd.close()
                    if dmn_grp in self.samplers and dmn in self.samplers[dmn_grp]:
                        fd = open(f'{local_path}/{dmn}.conf', 'w+')
                        # TO DO: Refactor sampler config architecture to more easily reference appropriate groups
                        self.write_samplers(fd, dmn_grp)
                        self.write_listeners(fd, dmn_grp, dmn, auth_list)
                    if fd:
                        fd.close()
                    print(f'Starting {dmn}')
                    start_args = self.ldmsd_arg_list(local_path, dmn_grp, dmn)
                    local_list[dmn] = subprocess.Popen(start_args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)
        if match_host is False:
            print(f'{hostname} does not match any daemon hosts in the ldms configuration file')
        for dmn in local_list:
            local_list[dmn].wait()

    def ldmsd_arg_list(self, local_path, dmn_grp, dmn):
        start_list = [ 'ldmsd' ]
        for ep in self.daemons[dmn_grp][dmn]['endpoints']:
            if self.daemons[dmn_grp][dmn]['endpoints'][ep]['maestro_comm'] is True:
                ep_ = self.daemons[dmn_grp][dmn]['endpoints'][ep]
                start_list.append('-x')
                start_list.append(f'{ep_["xprt"]}:{ep_["port"]}')
                auth = check_opt('auth', ep_)
                if auth:
                    auth_plugin = check_opt('plugin', ep_['auth'])
                    auth_opt = check_opt('conf', ep_)
                    start_list.append('-a')
                    start_list.append(auth_plugin)
                    if auth_opt:
                        if len(auth_opt.split('=')) < 2:
                            auth_opt = f'conf={auth_opt}'
                        start_list.append('-A')
                        start_list.append(auth_opt)
        start_list.append('-c')
        start_list.append(f'{local_path}/{dmn}.conf')
        start_list.append('-r')
        start_list.append(f'{local_path}/{dmn}.pid')
        start_list.append('-l')
        start_list.append(f'{local_path}/{dmn}.log')
        start_list.append(f'-F')
        return start_list

    def write_listeners(self, fd, dmn_grp, dmn_name, auth_list={}):
        for endp in self.daemons[dmn_grp][dmn_name]['endpoints']:
            ep = self.daemons[dmn_grp][dmn_name]['endpoints'][endp]
            if ep['maestro_comm'] is False:
                auth = check_opt('auth', ep)
                auth_opt = check_opt('conf', ep)
                if auth:
                    if auth not in auth_list:
                        auth_list[auth] = { 'conf' : auth_opt }
                        plugin = check_opt('plugin', ep['auth'])
                        fd.write(f'auth_add name={auth}')
                        self.write_opt_attr(fd, 'plugin', plugin, endline=False)
                        self.write_opt_attr(fd, 'conf', auth_opt)
                fd.write(f'listen xprt={ep["xprt"]} port={ep["port"]}')
                self.write_opt_attr(fd, 'auth', auth, endline=False)
                self.write_opt_attr(fd, 'conf', auth_opt)
        return auth_list

    def write_opt_attr(self, fd, attr, val, endline=True):
        # Include leading space
        if val is not None:
            fd.write(f' {attr}={val}')
        if endline:
            fd.write(f'\n')

    def write_producers(self, fd, group_name, dmn, auth_list):
        if group_name in self.producers:
            ''' Balance samplers across aggregators '''
            ppd = -(len(self.producers[group_name]) // -len(self.aggregators[group_name].keys()))
            rem = len(self.producers[group_name]) % len(self.aggregators[group_name].keys())
            prdcrs = list(self.producers[group_name].keys())
            aggs = list(self.daemons[group_name].keys())
            agg_idx = int(aggs.index(dmn))
            prdcr_idx = int(ppd * agg_idx)
            prod_group = prdcrs[prdcr_idx:prdcr_idx+ppd]
            i = 0
            auth = None
            for ep in prod_group:
                producer = self.producers[group_name][ep]
                auth = check_opt('auth', self.daemons[producer['dmn_grp']][producer['daemon']]['endpoints'][ep])
                auth_opt = check_opt('conf', self.daemons[producer['dmn_grp']][producer['daemon']]['endpoints'][ep])
                if auth not in auth_list:
                    auth_list[auth] = { 'conf' : auth_opt }
                    plugin = check_opt('plugin', self.daemons[producer['dmn_grp']][producer['daemon']]['endpoints'][ep]['auth'])
                    if plugin is None:
                        print(f'Please specify auth plugin type for producer "{producer["daemon"]}" with auth name "{auth}"\n'\
                               'configuration file generation will continue, but auth will likely be denied.\n')
                        plugin = auth
                    fd.write(f'auth_add name={auth} plugin={plugin}')
                    self.write_opt_attr(fd, 'conf', auth_list[auth]['conf'])
            for ep in prod_group:
                regex = False
                producer = self.producers[group_name][ep]
                pname = producer['name']
                port = self.daemons[producer['dmn_grp']][producer['daemon']]['endpoints'][ep]['port']
                xprt = self.daemons[producer['dmn_grp']][producer['daemon']]['endpoints'][ep]['xprt']
                hostname = self.daemons[producer['dmn_grp']][producer['daemon']]['addr']
                auth = check_opt('auth', self.daemons[producer['dmn_grp']][producer['daemon']]['endpoints'][ep])
                ptype = producer['type']
                interval = producer['reconnect']
                rail = check_opt('rail', producer)
                rx_rate = check_opt('rx_rate', producer)
                credits = check_opt('credits', producer)
                fd.write(f'prdcr_add name={pname} '+
                         f'host={hostname} '+
                         f'port={port} '+
                         f'xprt={xprt} '+
                         f'type={ptype} '+
                         f'interval={interval}')
                self.write_opt_attr(fd, 'auth', auth, endline=False)
                self.write_opt_attr(fd, 'rail', rail, endline=False)
                self.write_opt_attr(fd, 'rx_rate', rx_rate, endline=False)
                self.write_opt_attr(fd, 'credits', credits)
                last_sampler = pname
                if 'regex' in producer:
                    regex = True
                    fd.write(f'prdcr_start_regex regex={producer["regex"]}\n')
                if not regex:
                    fd.write(f'prdcr_start_regex regex=.*\n')
            return auth_list

    def write_samplers(self, fd, smplr_group):
        for sampler in self.samplers[smplr_group]['plugins']:
            sname = sampler['name']
            fd.write(f'load name={sname}\n')
            for cfg_ in sampler['config']:
                if type(cfg_) is dict:
                    hostname = socket.gethostname()
                    if args.local:
                        cfg_args = { 'producer'     : f'{hostname}',
                                     'instance'     : f'{hostname}/{sampler["name"]}',
                                     'component_id' : '${LDMS_COMPONENT_ID}' }
                    else:
                        cfg_args = {}
                    check_required(['producer', 'instance'], cfg_,
                                    'sampler plugin "config"')
                    for attr in cfg_:
                        if attr == 'name' or attr == 'interval':
                            continue
                    cfg_args[attr] = cfg_[attr]
                    cfg_str = parse_to_cfg_str(cfg_args)
                else:
                    cfg_str = cfg_

                interval = check_intrvl_str(sampler['interval'])
                fd.write(f'config name={sname} {cfg_str}\n')
            fd.write(f'start name={sname} interval={interval}')
            offset = check_opt('offset', sampler)
            self.write_opt_attr(fd, 'offset', offset)

    def write_stream_subscribe(self, fd, group_name, agg):
        subscribe = check_opt('subscribe', self.aggregators[group_name][agg])
        if subscribe:
            for stream in subscribe:
                regex = check_opt('regex', stream)
                rx_rate = check_opt('rx_rate', stream)
                if regex is None:
                    regex = '.*'
                fd.write(f'prdcr_subscribe stream={stream["stream"]} '\
                         f'regex={regex}')
                self.write_opt_attr(fd, 'rx_rate', rx_rate)

    def write_aggregators(self, path, group_name):
        # Agg config
        try:
            ''' "Balance" agg configuration if all samplers are included in each aggregator '''
            if group_name not in self.aggregators:
                return 0
            for agg in self.aggregators[group_name]:
                auth_list = {}
                fd = open(f'{path}/{agg}.conf', 'w+')
                auth_list = self.write_listeners(fd, group_name, agg, auth_list)
                auth_list = self.write_producers(fd, group_name, agg, auth_list)
                self.write_stream_subscribe(fd, group_name, agg)
                self.write_agg_plugins(fd, group_name, agg)
                self.write_updaters(fd, group_name)
                self.write_stores(fd, group_name)
        except Exception as e:
            ea, eb, ec = sys.exc_info()
            print('Agg config Error: '+str(e)+' Line:'+str(ec.tb_lineno))
            raise ValueError

    def write_agg_plugins(self, fd, group_name, agg):
        # Write independent plugin configuration for group <group_name>
        plugins = check_opt('plugins', self.aggregators[group_name][agg])
        if plugins is not None:
            for plugin in plugins:
                fd.write(f'load name={plugin["name"]}\n')
                for cfg_ in plugin["config"]:
                    if type(cfg_) is dict:
                        cfg_str = parse_to_cfg_str(plugin["config"])
                    else:
                        cfg_str = cfg_
                    fd.write(f'config name={plugin["name"]} {cfg_str}\n\n')

    def write_updaters(self, fd, group_name):
        if group_name in self.updaters:
            updtr_group = self.updaters[group_name]
            for updtr in updtr_group:
                interval = check_intrvl_str(updtr_group[updtr]['interval'])
                updtr_str = f'updtr_add name={updtr_group[updtr]["name"]} '
                if 'mode' in updtr_group[updtr]:
                    mode = updtr_group[updtr]['mode']
                else:
                    mode = 'pull'
                # Check mode
                if mode == 'push':
                    updtr_str = f'{updtr_str} push=True'
                elif mode == 'onchange':
                    updtr_str = f'{updtr_str} push=onchange'
                elif mode == 'auto_interval' or 'auto':
                    updtr_str = f'{updtr_str} auto_interval=True'
                fd.write(f'{updtr_str} '+
                         f'interval={interval}')
                offset = check_opt('offset', updtr_group[updtr])
                self.write_opt_attr(fd, 'offset', offset)
                for prod in updtr_group[updtr]['producers']:
                    fd.write(f'updtr_prdcr_add name={updtr_group[updtr]["name"]} '+
                             f'regex={prod["regex"]}\n')
                fd.write(f'updtr_start name={updtr_group[updtr]["name"]}\n')

    def write_stores(self, fd, group_name):
        if group_name in self.stores:
            store_group = self.stores[group_name]
            loaded_plugins = []
            for store in store_group:
                if store_group[store]["plugin"]["name"] not in loaded_plugins:
                    fd.write(f'load name={store_group[store]["plugin"]["name"]}\n')
                    for cfg_ in store_group[store]["plugin"]["config"]:
                        if type(cfg_) is dict:
                            cfg_str = parse_cfg_str(cfg_)
                        else:
                            cfg_str = cfg_
                        fd.write(f'config name={store_group[store]["plugin"]["name"]} '+
                                 f'{cfg_str}\n')
                    loaded_plugins.append(store_group[store]["plugin"]["name"])
                strgp_add = f'strgp_add name={store} plugin={store_group[store]["plugin"]["name"]} '
                strgp_add += f'container={store_group[store]["container"]} '
                strgp_add += f'schema={store_group[store]["schema"]}'
                fd.write(strgp_add)
                flush = check_opt('flush', store_group[store])
                self.write_opt_attr(fd, 'flush', flush)
                fd.write(f'strgp_start name={store}\n')

    def config_v4(self, path):
        """
        Read the group configuration from ETCD and generate a version 4 LDMSD configuration
        This configuration assumes that the environemnt variables COMPONENT_ID, HOSTNAME
        all exist on the machines relevant to the ldmsd cluster.
        """
        for group_name in self.daemons:
            # Sampler config
            if self.samplers != None:
                try:
                    # TO DO: Refactor sampler config architecture to more easily reference appropriate groups
                    if group_name in self.samplers:
                        fd = open(f'{path}/{group_name}-samplers.conf', 'w+')
                        self.write_samplers(fd, group_name)
                        for dmn_name in self.daemons[group_name]:
                            self.write_listeners(fd, group_name, dmn_name)
                    if fd:
                        fd.close()
                except Exception as e:
                    a, b, d = sys.exc_info()
                    print(f'Error generating sampler configuration: {str(e)} {str(d.tb_lineno)}')
                    sys.exit()
            else:
                print(f'"samplers" not found in configuration file. Skipping...')

            # Write aggregators in daemon group
            self.write_aggregators(path, group_name)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LDMS Monitoring Cluster Configuration")
    parser.add_argument("--ldms_config", metavar="FILE", required=True,
                        help="The ldmsd load balance domain configuration file. "
                        "This will not start the maestro "
                        "load balancer")
    parser.add_argument("--cluster", metavar="FILE",
                        help="The name of the etcd cluster configuration file")
    parser.add_argument("--prefix", metavar="STRING",
                        help="The prefix for the dumped aggregator configurations",
                        default="unknown")
    local = parser.add_mutually_exclusive_group()
    local.add_argument("--local", action='store_true', default=False, help="Start maestro_ctrl in local mode")
    parser.add_argument("--local_path", metavar="STRING",
                       help="The path for the generated local configuration files",
                       default=os.path.expanduser('~'), required=False)
    parser.add_argument("--generate-config-path", metavar="STRING", required=False,
                        default=False)
    parser.add_argument("--version", metavar="VERSION",
                        help="The OVIS version for the output syntax (4 or 5), default is 4",
                        default=4)
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug information")
    args = parser.parse_args()
    if not args.debug:
        import sys
        sys.tracebacklimit=0
    config_fp = open(args.ldms_config)
    conf_spec = yaml.safe_load(config_fp)

    if args.cluster:
        # All keys in the DB are prefixed with the prefix name. So we can
        # have multiple monitoring hosted by the same consensus cluster.
        import etcd3
        if not args.prefix:
            print(f'"prefix" is required when using etcd')
        # Load the cluster configuration file. This configures the daemons
        # that support the key/value configuration database
        etcd_fp = open(args.cluster)
        etcd_spec = yaml.safe_load(etcd_fp)

        etcd_hosts = ()
        for h in etcd_spec['members']:
            etcd_hosts += (( h['host'], h['port'] ),)

        # Use the 1st host for now
        client = etcd3.client(host=etcd_hosts[0][0], port=etcd_hosts[0][1],
            grpc_options=[ ('grpc.max_send_message_length',16*1024*1024),
                       ('grpc.max_receive_message_length',16*1024*1024)])
    else:
        client = None
        args.prefix = None

    cluster = ClusterCtrl(client, args.prefix, conf_spec, args)

    if args.local:
        cluster.local_mode(args.local_path)

    if args.generate_config_path:
        cluster.config_v4(args.generate_config_path)
        print("LDMSD v4 config files generated")
        sys.exit(0)

    # Replace existing configuration if etcd cluster specified
    if args.cluster:
        rc = cluster.save_config()
        if rc:
            print("Error saving ldms cluster configuration to etcd cluster.")
            sys.exit(0)
        print("LDMS cluster configuration saved to etcd cluster.")

    if not args.cluster and not args.prefix and not args.local and not args.generate_config_path:
        print(f'No action detected. Exiting...')

    sys.exit(0)
